{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6012f0f2-c115-4bec-ae4e-9abf20e3d432",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# InfluxDB to ADLS PoC Enhancement\n",
    "\n",
    "- This notebook is used to copy data stored in InfluxDB and send it to Azure Data Lake Storage Gen 2 in given format or as-is.\n",
    "Supported services: DFL, DataAPI\n",
    "- To send data as-is service name should be selected as Simple_JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6b8d8a-4dab-43b8-95e4-566aa6cfa2d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d401fa4-0e8c-4984-98c9-7d368e700bff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries for connecting to Azure Gen2 Storage and InfluxDB\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "import influxdb_client\n",
    "from dateutil.tz import tzlocal\n",
    "from azure.storage.filedatalake import (\n",
    "    DataLakeServiceClient,\n",
    "    DataLakeDirectoryClient,\n",
    "    FileSystemClient\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c8fcdb-9db0-4a6b-93d0-9cc6fa8e9227",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7203734-77e2-48f1-9ec2-391c1553f740",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-01T00:00:00Z 2017-02-02T00:00:00Z\n",
      "SOLAR DFL\n"
     ]
    }
   ],
   "source": [
    "tenant_name = dbutils.widgets.get(\"tenantName\")\n",
    "service_name = dbutils.widgets.get(\"serviceName\")\n",
    "start_at = dbutils.widgets.get(\"startDateTime\")\n",
    "stop_at = dbutils.widgets.get(\"endDateTime\")\n",
    "\n",
    "if 'T' not in start_at:\n",
    "    start_at = start_at + \"T00:00:00Z\"\n",
    "if 'T' not in stop_at:\n",
    "    stop_at = stop_at + \"T00:00:00Z\"\n",
    "\n",
    "print(start_at, stop_at)\n",
    "print(tenant_name,service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0493213-1862-4c9d-865a-4219d800863b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenant container exists\n"
     ]
    }
   ],
   "source": [
    "# Create a Data Lake Service Client\n",
    "account_url = f\"\"\n",
    "credential = \"\"\n",
    "client = DataLakeServiceClient(account_url, credential)\n",
    "\n",
    "# Check if the container corresponding to the tenant_name exists. If not, it creates the file system (container) for the specified tenant.\n",
    "file_system_client = client.get_file_system_client(tenant_name.lower())\n",
    "if not file_system_client.exists():\n",
    "    file_system_client.create_file_system()\n",
    "else:\n",
    "    print(\"Tenant container exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea27b28-46a2-467e-9dd8-ea6f1fd251f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if service_name == \"DFL_SimpleJSON\" or service_name == \"DFL\":\n",
    "    # DFL Config\n",
    "    bucket = \"\"\n",
    "    org = \"\"\n",
    "    token = \"\"\n",
    "    # Store the URL of your InfluxDB instance\n",
    "    url=\"\"\n",
    "elif service_name == \"DataAPI_SimpleJSON\" or service_name == \"DataAPI\":\n",
    "    # Data API Config\n",
    "    bucket = \"\"\n",
    "    org = \"\"\n",
    "    token = \"\"\n",
    "    # Store the URL of your InfluxDB instance\n",
    "    url=\"\"\n",
    "\n",
    "# Create InfluxDB client\n",
    "client = influxdb_client.InfluxDBClient(\n",
    "url=url,\n",
    "token=token,\n",
    "org=org\n",
    ")\n",
    "\n",
    "query_api = client.query_api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c891476-7614-40e9-9b8e-92d5ae5cedbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf2e6c2-0859-45fc-b81b-b3413ce753ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**1. Get Measurements List From InfluxDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05daad68-89e3-431f-b484-e3bbe57d7ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get measurements list\n",
    "def get_measurement_list(bucket, start_at, stop_at):\n",
    "    \"\"\"\n",
    "    Retrieves a list of unique measurements from a specified InfluxDB bucket within a given time range.\n",
    "    Args:\n",
    "    - bucket: The name of the InfluxDB bucket to query.\n",
    "    - start_at: The start time for the query range.\n",
    "    - stop_at: The stop time for the query range.\n",
    "    Returns:\n",
    "    - A list of unique measurement names found within the specified time range.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        import \"date\"         \n",
    "        from(bucket: \"{bucket}\")         \n",
    "        |> range(start: {start_at}, stop: {stop_at})         \n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")         \n",
    "        |> keep(columns: [\"_measurement\"])         \n",
    "        |> distinct()\n",
    "        \"\"\"\n",
    "    result = query_api.query(query=query)\n",
    "    measurement_list = []\n",
    "    for table in result:\n",
    "        for record in table.records:\n",
    "            if record[\"_measurement\"] not in measurement_list:\n",
    "                measurement_list.append(record[\"_measurement\"])\n",
    "    \n",
    "    return measurement_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc08d22e-c24e-4281-853f-2c629af4341d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**2. Query Data For A Specific Measurement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461380b6-87c1-420a-9345-1c27e6a85a76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def query_measurement(bucket, measurement, start_at, stop_at):\n",
    "    \"\"\"\n",
    "    Queries a specific measurement within a specified InfluxDB bucket and time range.\n",
    "    Args:\n",
    "    - bucket: The name of the InfluxDB bucket to query.\n",
    "    - service_name: The name of the service with options: DFL, DFL_SimpleJSON, DataAPI, DataAPI_SimpleJSON\n",
    "    - measurement: The name of the measurement to query.\n",
    "    - start_at: The start time for the query range.\n",
    "    - stop_at: The stop time for the query range.\n",
    "    Returns:\n",
    "    - The result of the query for the specified measurement within the given time range.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        import \"date\"         \n",
    "        from(bucket: \"{bucket}\")         \n",
    "        |> range(start: {start_at}, stop: {stop_at})\n",
    "        |> filter(fn: (r) => r[\"_measurement\"] == \"{measurement}\")         \n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")         \n",
    "        |> drop(columns: [\"_start\", \"_stop\"])\n",
    "        \"\"\"\n",
    "    result = query_api.query(query=query)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de062f5-dacc-4f9b-9a9f-2e7f3a334ee7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**3. Process Data to Generate JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d46489-6df7-4975-bde7-878222ffd977",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_data(record):\n",
    "    \"\"\"\n",
    "    Process the input record and generate a JSON data structure along with a file path based on the selected service. \n",
    "    Args:\n",
    "    record (dict): The input record containing data to be processed. \n",
    "    Returns:\n",
    "    list: A list containing the file path and the generated JSON data structure.\n",
    "    \"\"\"\n",
    "    if service_name == \"DFL\":      \n",
    "        atts_arr = []\n",
    "        att_curr = None\n",
    "        json_data = {\"Value\": record[\"data\"], \"Timestamp\": record[\"_time\"].isoformat()}\n",
    "        # Check each key       \n",
    "        for key, value in record.values.items() :           \n",
    "            if key.startswith(\"attr_\") :\n",
    "                att = key.split('_')                   \n",
    "                # Add attribute name and value      \n",
    "                if att_curr != att[1] and value is not None:\n",
    "                    atts = {\"Name\": att[1], \"Value\": value, \"SemanticRef\": None}\n",
    "                    atts_arr.append(atts)\n",
    "                    att_curr = att[1]                   \n",
    "                # Add semantic ref to the last added attribute\n",
    "                elif att_curr == att[1] and att[2] == \"semanticref\" and atts_arr:\n",
    "                    atts_arr[-1][\"SemanticRef\"] = value\n",
    "        if atts_arr:\n",
    "            json_data[\"Attributes\"] = atts_arr\n",
    "\n",
    "    elif service_name == \"DataAPI\":\n",
    "        with open('/dbfs/FileStore/schema.json', 'r') as file:\n",
    "            schema = json.load(file)\n",
    "        # print(record.values)\n",
    "        # Extracting field and attribute names from the schema\n",
    "        sch_fields = [e[\"name\"] for e in schema[\"fields\"]]\n",
    "        sch_atts = [a[\"name\"] for a in schema[\"attributes\"]]\n",
    "\n",
    "        # Initializing output lists\n",
    "        out_fields = []\n",
    "        out_atts = []\n",
    "\n",
    "        # Creating the data dictionary\n",
    "        data = {\"timestamp\": record[\"_time\"].isoformat()}\n",
    "        for key, value in record.values.items():               \n",
    "            if key in sch_atts:\n",
    "                out_atts.append({\"name\": key, \"value\": value})\n",
    "            elif key in sch_fields:\n",
    "                out_fields.append({\"name\": key, \"value\": value})\n",
    "\n",
    "        # Adding fields and attributes to the data dictionary if they exist\n",
    "        if out_fields:\n",
    "            data[\"fields\"] = out_fields\n",
    "        if out_atts:\n",
    "            data[\"attributes\"] = out_atts\n",
    "\n",
    "        # Creating the final output JSON\n",
    "        json_data = {\"data\": data}\n",
    "\n",
    "    # If service name: DFL_SimpleJSON or DataAPI_SimpleJSON\n",
    "    else:\n",
    "        data = record.values\n",
    "        print(record.values)\n",
    "        data.pop(\"result\")\n",
    "        data.pop(\"table\")\n",
    "        data[\"_time\"] = record.values.get(\"_time\").isoformat()\n",
    "        json_data = {key: value for key, value in data.items() if value is not None}\n",
    "    \n",
    "    # Create a specific path for the telemetry data\n",
    "    storage = record.values.get(\"_measurement\")\n",
    "    deviceId = record.values.get(\"series_id\")\n",
    "    # Example time: \"2024-01-26T06:12:00+00:00\"\n",
    "    timestamp = record.values.get(\"_time\").isoformat()\n",
    "    year, month, day, hour, minute = timestamp.split('T')[0].split('-') + timestamp.split('T')[1].split(':')[:2]\n",
    "    file_path = f\"/staging/influx/{service_name}/telemetry/{storage}/{deviceId}/{year}/{month}/{day}/{hour}/{minute}\"\n",
    "    \n",
    "    return [file_path, json_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc6226e8-0788-4497-b1fb-b384bfa9aeb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**4. Send JSON Data to Azure Data Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e6b8bc-24ef-462d-9cb2-ce62ca140ca9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def send_to_ADLS(client, file_path, json_data):\n",
    "    \"\"\"\n",
    "    Send the JSON data to Azure Data Lake Storage (ADLS) at the specified file path.  \n",
    "    Args:\n",
    "    client: The client for accessing Azure Data Lake Storage.\n",
    "    file_path (str): The path in ADLS where the JSON data will be stored.\n",
    "    json_data (dict): The JSON data to be stored in ADLS.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the directory system client for the specified file path\n",
    "    directory_system_client = file_system_client.get_directory_client(file_path)\n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not directory_system_client.exists():\n",
    "        directory_system_client.create_directory()\n",
    "        print(\"Path created.\")\n",
    "    else:\n",
    "        print(\"Specified path exists.\")\n",
    "\n",
    "    # Check if the file already exists  \n",
    "    file_client= directory_system_client.get_file_client('telemetry.json')\n",
    "    if not file_client.exists():\n",
    "        # If the file does not exist, upload the JSON data as a new file\n",
    "        json_data = json.dumps([json_data]).encode(\"utf-8\")\n",
    "        file_client.upload_data(json_data, overwrite= True)\n",
    "        print(\"File created.\")\n",
    "    else:\n",
    "        # If the file exists, download the existing data and append the new JSON data\n",
    "        read_json_data = file_client.download_file()\n",
    "        read_json_data = json.loads(read_json_data.readall())\n",
    "        data = [json_data]\n",
    "        # Compare timestamps and update existing data accordingly\n",
    "        if service_name == \"DFL\":\n",
    "            for i,entry in enumerate(read_json_data):\n",
    "                if data[0][\"Timestamp\"]== entry[\"Timestamp\"]:\n",
    "                    entry = data[0]\n",
    "                    read_json_data[i] = entry\n",
    "                    append_data = read_json_data\n",
    "                    break\n",
    "                else:\n",
    "                    append_data = data + read_json_data\n",
    "        elif service_name == \"DataAPI\" :\n",
    "            for i,entry in enumerate(read_json_data):\n",
    "                if data[0][\"data\"][\"timestamp\"]== entry[\"data\"][\"timestamp\"]:\n",
    "                    entry = data[0]\n",
    "                    read_json_data[i] = entry\n",
    "                    append_data = read_json_data\n",
    "                    break\n",
    "                else:\n",
    "                    append_data = data + read_json_data\n",
    "        else :\n",
    "            for i,entry in enumerate(read_json_data):\n",
    "                if data[0][\"_time\"]== entry[\"_time\"]:\n",
    "                    entry = data[0]\n",
    "                    read_json_data[i] = entry\n",
    "                    append_data = read_json_data\n",
    "                    break\n",
    "                else:\n",
    "                    append_data = data + read_json_data\n",
    "        # Upload the updated JSON data to the file in ADLS\n",
    "        append_data = json.dumps(append_data).encode(\"utf-8\")\n",
    "        file_client.upload_data(append_data, overwrite= True)\n",
    "        print(\"Data appended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f88241-d24b-4486-8140-3171a538aa1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0104af-7c30-4518-a3a1-7b44826e38a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFL\n"
     ]
    }
   ],
   "source": [
    "# Start migration\n",
    "def main():\n",
    "    \n",
    "    print(service_name)\n",
    "    measurement_list = get_measurement_list(bucket, start_at, stop_at)\n",
    "    \n",
    "    process_data_list = []\n",
    "    for measurement in measurement_list:\n",
    "        result = query_measurement(bucket, measurement, start_at, stop_at)\n",
    "        for table in result:\n",
    "            for record in table.records:\n",
    "                [file_path, json_data]  = process_data(record)\n",
    "                process_data_list.append((file_path, json_data))\n",
    "                # send_to_ADLS(client, file_path, json_data)\n",
    "    # print(service_name + \" data upload completed.\")\n",
    "    return process_data_list\n",
    "\n",
    "process_data_list = main()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42ed3749-323d-4baa-9bb9-b98ac26df9e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce810ebd-4f9a-43d6-a2f9-b08daccb285e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified path exists\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n",
      "Specified path exists\n",
      "Specified path exists\n",
      "Data appended.\n",
      "Data appended.\n"
     ]
    }
   ],
   "source": [
    "# Upload files in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:  # Adjust max_workers as needed\n",
    "    executor.map(lambda args: send_to_ADLS(client, *args), process_data_list)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "tenantName",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "serviceName",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "startDateTime",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "endDateTime",
      "width": 206
     }
    ]
   },
   "notebookName": "Timeseries_PoC_Enhancement",
   "widgets": {
    "endDateTime": {
     "currentValue": "2017-02-02",
     "nuid": "b80c0796-6cbe-4b4f-9eac-55d587f23fab",
     "widgetInfo": {
      "defaultValue": "2017-01-02",
      "label": "",
      "name": "endDateTime",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "serviceName": {
     "currentValue": "DFL",
     "nuid": "6c078906-6675-4a80-9a0c-6bfbfcb07c54",
     "widgetInfo": {
      "defaultValue": "DFL",
      "label": "",
      "name": "serviceName",
      "options": {
       "autoCreated": false,
       "choices": [
        "DFL",
        "DataAPI",
        "DFL_SimpleJSON",
        "DataAPI_SimpleJSON"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "startDateTime": {
     "currentValue": "2017-02-01",
     "nuid": "56cf7264-0126-48a3-a42a-5b51a4dba187",
     "widgetInfo": {
      "defaultValue": "2017-01-01",
      "label": "",
      "name": "startDateTime",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "tenantName": {
     "currentValue": "SOLAR",
     "nuid": "8f05d897-f901-43b6-a8f6-e89d27ecbfc0",
     "widgetInfo": {
      "defaultValue": "SOLAR",
      "label": "",
      "name": "tenantName",
      "options": {
       "autoCreated": false,
       "choices": [
        "dflqa",
        "SOLAR",
        "ETPQA1"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
